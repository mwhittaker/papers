<!DOCTYPE html>
<html>
<head>
  <title>Papers</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Papers</a>
  </div>
  <div id="container">
<h2 id="storm-twitter-2014"><a href="https://scholar.google.com/scholar?cluster=17632042616176969463&amp;hl=en&amp;as_sdt=0,5">Storm @ Twitter (2014)</a></h2>
<p>Storm is Twitter's stream processing system designed to be scalable, resilient, extensible, efficient, and easy to administer. In Storm, streams of tuples flow through (potentially cyclic) directed graphs, called <em>topologies</em>, of processing elements. Each processing element is either a <em>spout</em> (a source of tuples) or a <em>bolt</em> (a tuple processor).</p>
<p><strong>Storm Overview.</strong> Storm runs on a cluster, typically over something like Mesos. Each Storm cluster is managed by a single master node knows as a Nimbus. The Nimbus oversees a cluster of workers. Each worker runs multiple worker processes which run a JVM which run multiple executors which run multiple tasks:</p>
<pre><code>worker
+--------------------------------------------------------------+
| worker process                 worker process                |
| +---------------------------+  +---------------------------+ |
| | JVM                       |  | JVM                       | |
| | +-----------------------+ |  | +-----------------------+ | |
| | | executor   executor   | |  | | executor   executor   | | |
| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | | |task| | | |task| | | |  | | | |task| | | |task| | | | |
| | | | +----+ | | +----+ | | |  | | | +----+ | | +----+ | | | |
| | | +--------+ +--------+ | |  | | +--------+ +--------+ | | |
| | +-----------------------+ |  | +-----------------------+ | |
| +---------------------------+  +---------------------------+ |
| supervisor                                                   |
| +----------------------------------------------------------+ |
| |                                                          | |
| +----------------------------------------------------------+ |
+--------------------------------------------------------------+</code></pre>
<p>Users specify a topology which acts as a logical topology. Storm exploits data parallelism by expanding the logical topology into a physical topology in which each logical bolt is converted into a replicated set of physical bolts. Data is partitioned between producer and consumer bolts using one of the following partitioning scheme:</p>
<ul>
<li><em>shuffle</em>: Data is randomly shuffled.</li>
<li><em>fields</em>: Data is hash partitioned on a subset of fields.</li>
<li><em>all</em>: All data is sent to all downstream bolts.</li>
<li><em>global</em>: All data is sent to a single bolt.</li>
<li><em>local</em>: Data is sent to a task running on the same executor.</li>
</ul>
<p>Each worker runs a <em>supervisor</em> which communicates with the Nimbus. The Nimbus stores its state in Zookeeper.</p>
<p><strong>Storm Internals.</strong></p>
<p><em>Nimbus and Zookeeper.</em> In Storm, topologies are represented as Thrift objects, and the Nimbus is a Thrift server which stores topologies in Zookeeper. This allows topologies to be constructed in any programming language or framework. For example, Summingbird is a Scala library which can compile dataflows to one of many data processing systems like Storm or Hadoop. Users also send over a JAR of the code to the Nimbus which stores it locally on disk. Supervisors advertise openings which the Nimbus fills. All communication between workers and the Nimbus is done through Zookeeper to increase the resilience of the system.</p>
<p><em>Supervisor.</em> Each worker runs a supervisor process which is responsible for communicating with the Nimbus, spawning workers, monitoring workers, restarting workers, etc. The supervisor consists of three threads: (1) a <em>main thread</em>, (2) an <em>event manager thread</em>, and (3) a <em>process event manager thread</em>. The main thread sends heartbeats to the Nimbus every 15 seconds. The event manager thread looks for assignment changes every 10 seconds. The process event manager thread monitors workers every 3 seconds.</p>
<p><em>Workers and Executors.</em> Each executor is a thread running in a JVM. Each worker process has a thread to receive tuples and thread to send tuples. The receiving thread multiplexes tuples to different tasks' input queues. Each executor runs (1) a <em>user logic thread</em> which reads tuples from its input queue and processes them and (2) an <em>executor send thread</em> which puts outbound tuples in a global outbound queue.</p>
<p><strong>Processing Semantics.</strong> Storm provides <em>at most once</em> and <em>at least once</em> semantics. Each tuple in the system is assigned a unique 64 bit identifier. When a bolt processes a tuple, it can generate new tuples. Each of these tuples is also given a unique identifier. The lineage of each tuple is tracked in a lineage tree. When a tuple leaves the system, all bolts that contributed to it are acknowledged and can retire their buffered output. Storm implements this using a memory-efficient representation that uses bitwise XORs.</p>
<p><strong>Commentary.</strong> The paper doesn't mention stateful operators.</p>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
</body>
</html>
